[{"symbol": "AVGO", "quarter": 4, "year": 2025, "date": "2025-12-11 17:00:00", "content": "Ji Yoo: Welcome to Broadcom Inc.'s Fourth Quarter and Fiscal Year 2025 Financial Results Conference Call. At this time, for opening remarks and introductions, I would like to turn the call over to Ji Yoo, Head of Investor Relations of Broadcom Inc. Thank you, Sherry, and good afternoon, everyone. Joining me on today's call are Hock Tan, President and CEO, Kirsten Spears, Chief Financial Officer, and Charlie Coaz, President, Semiconductor Solutions Group. Broadcom distributed a press release and financial tables after the market closed describing our financial performance for the fourth quarter and fiscal year 2025. If you did not receive a copy, you may obtain the information from the investor section of Broadcom's website at broadcom.com. This conference call is being webcast live, and an audio replay of the call can be accessed for one year through the Investors section of Broadcom's website. During the prepared remarks, Hock and Kirsten will be providing details of our fourth quarter and fiscal year 2025 results, guidance for 2026, as well as commentary regarding the business environment. We will take questions after the end of our prepared comments. Please refer to our press release today and our recent filings with the SEC for information on the specific risk factors that could cause our actual results to differ materially from the forward-looking statements made on this call. In addition to U.S. GAAP reporting, Broadcom reports certain financial measures on a non-GAAP basis. A reconciliation between GAAP and non-GAAP measures is included in the tables attached to today's press release. Comments made during today's call will primarily refer to our non-GAAP financial results. I'll now turn the call over to Hock.\nHock Tan: And thank you everyone for joining us today. Well, we just ended our Q4 fiscal 2025. And before I get into details of that quarter, let me recap the year. In our fiscal 2025, consolidated revenue grew 24% year over year, to a record $64 billion, driven by AI semiconductors and VMware. AI revenue grew 65% year over year to $20 billion, driving the semiconductor revenue for this company to a record $37 billion for the year. In our infrastructure software business, strong adoption of VMware Cloud Foundation, or VCF as we call it, drove revenue growth of 26% year on year to $27 billion. In summary, 2025 was another strong year for Broadcom. And we see the spending momentum by our customers for AI continuing to accelerate in 2026. Now let's move on to the results of our fourth quarter 2025. Total revenue was a record $18 billion, up 28% year on year and above our guidance on better than expected growth in AI semiconductors as well as infrastructure software. Q4 consolidated adjusted EBITDA was a record $12.12 billion, up 34% year on year. So let me give you more color on our two segments. In semiconductors, revenue was $11.1 billion as year on year growth accelerated to 35%. This robust growth was driven by AI semiconductor revenue of $6.5 billion, which was up 74% year on year. This represents a growth trajectory exceeding 10 times over the eleven quarters we have reported this line of business. Our customer accelerated business more than doubled year over year as we see our customers increase adoption of XPUs, as we call those customer accelerators, in training their LLMs and monetizing their platforms through inferencing APIs and applications. These XPUs, I may add, are not only being used to train and inference internal workloads by our customers. The same experience in some situations has been extended externally to other LLM peers. Best exemplified at Google where the TPUs used in creating Gemini are also being used for AI cloud computing by Apple, Cohere, and SSI as a sample. The scale at which we see this happening could be significant. As you are aware, last quarter, Q3 2025, we received a $10 billion order to sell the latest TPU ironwood racks to Anthropic. This was our fourth custom. That we mentioned. In this quarter Q4, we received an additional $11 billion order from this same customer for delivery in late 2026. But that does not mean our other two customers are using TPUs. In fact, they prefer to control their own destiny by continuing to drive their multiyear journey to create their own custom AI accelerators or XPU RECs as we call them. I am pleased today to report that during this quarter, we acquired a fifth XPU customer through a $1 billion order placed for delivery in late 2026. Now moving on to AI networking. Demand here has even been stronger as we see customers build out their data center infrastructure ahead of deploying AI accelerators. Our current order backlog for AI switches exceeds $10 billion as our latest 102 terabyte terabit per second Tomahawk six switch, the first and only one of its capability out there, continues to book at record rates. This is just a subset of what we have. We have also secured record orders on DSPs, optical components like lasers, and PCI Express switches to be deployed in AI data centers. All these components combined with our XPUs bring our total order on hand in excess of $73 billion today, which is almost half Broadcom's consolidated backlog of $162 billion. We expect this $73 billion in AI backlog to be delivered over the next eighteen months. In Q1 fiscal 2026, we expect our AI revenue to double year on year to $8.2 billion. Turning to non-AI semiconductors, Q4 revenue of $4.6 billion was up 2% year on year and up 16% sequentially based on favorable wireless seasonality. Year on year, broadband showed solid recovery. Wireless was flat. All the other end markets were down as enterprise spending continued to show limited signs of recovery. Accordingly, in Q1, we forecast non-AI semiconductor revenue to be approximately $4.1 billion, flat from a year ago, down sequentially due to wireless seasonality. Let me now talk about our infrastructure software segment. Q4 Infrastructure Software revenue of $6.9 billion was up 19% year on year, both and above our outlook of 6.7. Bookings continued to be strong, as total contract value booked in Q4 exceeded $10.4 billion versus $8.2 billion a year ago. We ended the year with $73 billion of infrastructure software backlog, up from $49 billion a year ago. We expect renewals to be seasonal in Q1 and forecast infrastructure software revenue to be approximately $6.8 billion. We still expect, however, that for fiscal 2026, Infrastructure Software revenue to grow low double-digit percentage. So here's what we see in 2026. Directionally, we expect AI revenue to continue to accelerate and drive most of our growth. Non-AI semiconductor revenue to be stable. Infrastructure software revenue will continue to be driven by VMware growth at low double digits. For Q1 2026, we expect consolidated revenue of approximately $19.1 billion, up 28% year on year. We expect adjusted EBITDA to be approximately 67% of revenue. With that, let me turn the call over to Kirsten.\nKirsten Spears: Thank you, Hock. Let me now provide additional detail on our Q4 financial performance. Consolidated revenue was a record $18 billion for the quarter, up 28% from a year ago. Gross margin was 77.9% of revenue in the quarter, better than we originally guided on higher software revenues and product mix within semiconductors. Consolidated operating expenses were $2.1 billion, of which $1.5 billion was research and development. Q4 operating income was a record $11.9 billion, up 35% from a year ago. Now on a sequential basis, even as gross margin was down 50 basis points on semiconductor product mix, operating margin increased 70 basis points sequentially to 66.2% on favorable operating leverage. Adjusted EBITDA of $12.12 billion or 68% of revenue was above our guidance of 67%. This figure excludes $148 million of depreciation. Now a review of the P&L for our two segments. Starting with semiconductors. Revenue for our Semiconductor Solutions segment was a record $11.1 billion with growth accelerating to 35% year on year driven by AI. Semiconductor revenue represented 61% of total revenue in the quarter. Gross margin for our Semiconductor Solutions segment was approximately 68%. Operating expenses increased 16% year on year, to $1.1 billion on increased investment in R&D for leading-edge AI semiconductors. Semiconductor operating margin of 59% was up 250 basis points year on year. Now moving to infrastructure software. Revenue for Infrastructure Software of $6.9 billion was up 19% year on year and represented 39% of total revenue. Gross margin for infrastructure software was 93% in the quarter compared to 91% a year ago. Operating expenses were $1.1 billion in the quarter, resulting in Infrastructure Software operating margin of 78%. This compares to operating margin of 72% a year ago reflecting the completion of the integration of VMware. Moving on to cash flow. Free cash flow in the quarter was $7.5 billion and represented 41% of revenue. We spent $237 million on capital expenditures. Day sales outstanding were thirty-six days in the fourth quarter, compared to twenty-nine days a year ago. We ended the fourth quarter with inventory of $2.3 billion, up 4% sequentially. Our days of inventory on hand were fifty-eight days in Q4, compared to sixty-six days in Q3 as we continue to remain disciplined on how we manage inventory across the ecosystem. We ended the fourth quarter with $16.2 billion of cash, up $5.5 billion sequentially on strong cash flow generation. The weighted average coupon rate in years to maturity of our gross principal fixed rate debt of $67.1 billion is 4% and 7.2 years, respectively. Turning to capital allocation. In Q4, we paid stockholders $2.8 billion of cash dividends based on a quarterly common stock cash dividend of $0.59 per share. In Q1, we expect the non-GAAP diluted share count to be 4.97 billion shares excluding the potential impact of any share repurchases. Now let me recap our financial performance for fiscal year 2025. Our revenue hit a record $63.9 billion with organic growth accelerating to 24% year on year. Semiconductor revenue was $36.9 billion, up 22% year over year. Infrastructure software revenue was $27 billion, up 26% year on year. Fiscal 2025 adjusted EBITDA was $43 billion and represented 67% of revenue. Free cash flow grew 39% year on year to $26.9 billion. For fiscal 2025, we returned $17.5 billion of cash to shareholders in the form of $11.1 billion of dividends and $6.4 billion in share repurchases and eliminations. Aligned with our ability to generate increased cash flows in the preceding year, we are announcing an increase in our quarterly common stock cash dividend in Q1 fiscal 2026 to 65\u00a2 per share, an increase of 10% from the prior quarter. We intend to maintain this target quarterly dividend throughout fiscal 2026, subject to quarterly board approval. This implies our fiscal 2026 annual common stock dividend to be a record $2.60 per share, an increase of 10% year on year. I would like to highlight that this represents the fifteenth consecutive increase in annual dividends since we initiated dividends in fiscal 2011. The board also approved an extension of our share repurchase program of which $7.5 billion remains, through the end of calendar year 2026. Now moving to guidance. Our guidance for Q1 is for consolidated revenue of $19.1 billion, up 28% year on year. We forecast semiconductor revenue of approximately $12.3 billion, up 50% year on year. Within this, we expect Q1 AI semiconductor revenue of $8.2 billion, up approximately 100% year on year. We expect infrastructure software revenue of approximately $6.8 billion, up 2% year on year. For your modeling purposes, we expect Q1 consolidated gross margin to be down approximately 100 basis points sequentially, primarily reflecting a higher mix of AI revenue. As a reminder, consolidated gross margins through the year will be impacted by the revenue mix of infrastructure software and semiconductors and also product mix within semiconductor. Expect Q1 adjusted EBITDA to be approximately 67%. We expect the non-GAAP tax rate for Q1 and fiscal year 2026 to increase from 14% to approximately 6.5% due to the impact of the global minimum tax and shift in geographic mix of income compared to that of fiscal year 2025. That concludes my prepared remarks. Operator, please open up the call for questions.\nOperator: Thank you. Due to time restraints, we ask that you please limit yourself to one question. Please stand by while we come to our first question.\nVivek Arya: Our first question will come from the line of Vivek Arya with Bank of America. Your line is open.\nHock Tan: Thank you. Just wanted to clarify, Hock, you said $73 billion over eighteen months for AI. That's, you know, roughly $50 billion plus for fiscal 2026. For AI. I just wanted to make sure I got that right. And then the main question, Hock, is that there is sort of this emerging debate about customer-owned tooling. You know, your async customers potentially wanting to do more things on their own. How do you see your XPU content and share at your largest customer evolve over the next one or two years? Thank you.\nHock Tan: Well, to answer your first question, what we said is correct that as of now, we have $73 billion of backlog in place, account of XPU switches, DSPs, lasers, for AI data centers that we anticipate shipping over the next eighteen months. And obviously, this is as of now. I mean, we fully expect more bookings to come in over that period of time. And so do not take that $73 billion as that's the revenue we ship over the next eighteen months. Just saying we have that now, and then the bookings have been accelerating. And frankly, we see that bookings not just in XPUs, but in switches, DSPs, all the other components that go into AI data centers. We have never seen bookings of the nature that what we have seen over the past three months. Particularly, with respect to Taiwan six switches. This is one of the fastest-growing products in terms of deployment that we have ever seen. Of any switch product that we put out there. It is pretty interesting. And partly because it's the only one of its kind out there at this point. At 102 terabit per second. And that's the exact product needed to expand the clusters of the latest GPU and XPUs out there. No. That's great. But as far as what is the future as XPU is your broader question. My answer to you is do not follow what you hear out there as gospel. It's a trajectory. It's a multiyear journey. And many of the players and not too many players doing LLMs want to do their own custom AI accelerator for very good reasons. You can put in hardware what if you use a general-purpose GPU you can only do in software and kernels in software. You can achieve performance-wise so much better in the custom-purpose design hardware-driven XPU. And we see that in the TPU and we see that in all the accelerators we are doing for other customers. Much much better in areas of sparse call, training, inference, reasoning, all that stuff. Now will that mean that over time, they all want to go do it themselves?\nOperator: Not necessarily.\nHock Tan: And in fact, because the technology in silicon keeps updating, keeps evolving. And if you are an LLM player, where do you put your resources in order to compete in this space? Especially when you have to compete at the end of the day against merchant GPU. Who are not slowing down in their rate of evolution. So I see that as this concept of customer tooling is an overblown hypothesis which frankly I do not think will happen. Thank you.\nOperator: And that will come from the line of Ross Seymore with Deutsche Bank. Your line is open.\nRoss Seymore: Hi, thanks for letting me ask a question. Hock, I wanted to go to something you touched on earlier about the TPUs going a little bit more to like a merchant go-to-market to other customers. Do you believe that's the substitution effect for customers who otherwise would have done ASICs with you, or do you think it's actually broadening the market? And so what are kind of the financial implications of that from your perspective?\nHock Tan: That's a very good question, Ross. And what we see right now is the most obvious move it does is it goes to the people who use TPUs. The alternative is GPUs, merchant basis. That's the most common thing that happens. Because to do that substitution for another custom, it's different. To make an investment in a custom accelerator is a multiyear journey. It's a strict directional thing. It's not necessarily a very transactional or short-term move. Moving from GPU to TPU is a transactional move. Going into an AI accelerator of your own is a long-term strategic move and nothing would deter you from there to continue to make that investment towards that end goal of successfully creating and deploying your own custom AI accelerator. So that's the motion we see.\nRoss Seymore: Thank you.\nOperator: And that will come from the line of Harlan Sur with JPMorgan.\nHarlan Sur: Yes, good afternoon. Thanks for taking my question and congratulations on the strong results, guidance, and execution. Hock, again, I just want to sort of verify this. Right? So you talked about total AI backlog of $73 billion over the next six quarters. Right? This is just a snap of your order book, like, right now. But given your lead times, I think customers can and still will place orders for AI in quarters four, five, and six. So as time moves forward, that backlog number for more shipments in '26 will probably still go up. Right? Is that the correct interpretation? And then given the strong and growing backlog, right, the question is does the team have three nanometer, two nanometer wafer supply, COA substrate, HBM supply commitments to support all of the demand in your order book and I know one of the areas where you are trying to mitigate this is in advanced packaging. Right? You're bringing up your Singapore facility. You guys just remind us what part of the advanced packaging process the team is focusing on with the Singapore facility? Thanks.\nHock Tan: Thanks. Well, to answer your first simpler question, you're right. You can say that $73 billion is the backlog we have today. To ship over the next six quarters. You might also say that and given our lead time, we expect more orders to be able to be absorbed into our backlog for shipments over the next six quarters. So take it that we expect revenue a minimum revenue, one way to look at it, of $73 billion over the next six quarters. But we do expect much more as more orders come in. For shipments within the next six quarters. Yeah. Our lead time depending on the particular product it is, can be anywhere from six months to a year. On with respect to supply chain is what you're asking critical supply chain on silicon. Yeah. And packaging. Yeah. That's an interesting challenge that we have been addressing over for constantly and continue to. And with the strength of the demand and the need for more innovative packaging, advanced packaging, because you are talking about multi chips multi multi chips in creating every customer accelerator now. The packaging becomes a very interesting and technical challenge. And building our Singapore fab is to really talk about partially insourcing those advanced packaging. We believe that we have enough demand we can literally insource not from the viewpoint of not just cost, but in the viewpoint of supply chain security and delivery. And we're building up a fairly substantial facility for packaging advanced packaging, Singapore as indicated. Purely for that purpose to address the package advanced packaging site. Silicon wise, now we go back to the same pressure source in Taiwan, TSMC. And so we keep going for more and more capacity in two nanometers, three nanometers, and so far, we do not have that constraint. But again, time will tell as we progress and as our backlog builds up. Thank you.\nOperator: The next question will come from the line of Blayne Curtis with Jefferies. Your line is open.\nBlayne Curtis: Hey, good afternoon. Thanks for taking my question. I wanted to ask with the original $10 billion deal, you talked about a rack sale. I just wanted to with the follow-on order as well as the fifth customer, can you just maybe describe how you're gonna deliver those? Is it an XPU, or is it a rack? And then maybe you can kinda just walk us through the math and kinda what the deliverable is. Obviously, Google uses its own networking, so I'm kinda curious too. Would it be a copy exactly what Google does? That you could talk to it to name? Or would you have your own networking there as well?\nHock Tan: Thanks. That's a very complicated question, Blayne. Let me tell you what it is. It's a system sale. How about that? It's a real system sale. We have so many components beyond XPUs, customer accelerators, in or any system, in AI system, any AI system, used by hyperscalers that, yeah, we believe it begins to make sense to do it as a system sales and be responsive but be fully responsible for the entire system or rack as you call it. I think people understand it as a system still better. And so on this customer number four, we are selling it as a system with our key components in it. And that's no different than selling a chip. We certify a final ability to run as part of the wholesale selling process.\nOperator: And that will come from the line of Stacy Rasgon with Bernstein. Your line is open.\nStacy Rasgon: Wanted to touch on gross margins and maybe it feeds into a little bit of prior question. So I understand why the AI business is somewhat dilutive to gross margins. We have the HPM pass-through. And then presumably with the system sales, that will be more diluted. And you've hinted at this in the past, but I was wondering if you could be a little more explicit. As this AI revenue starts to ramp, as we start to get system sales, how should we be thinking about that gross margin number, say, if we're looking out, you know, four quarters or six quarters, is it low 70s? I mean, could it start with a six at the corporate level? And I guess I'm also wondering I understand how that comes down, but what about the operating margins? Do you think you get enough operating leverage on the OpEx side to keep operating margins flat, or do they need to come down as well?\nHock Tan: I'll let Kirsten give you the details, enough for me to broadly high level explain to you stating good question. Phenomenal. You don't see that impacting us right now, and we have already started that process. Of some system sales. You don't see that in our numbers, but it worked. And we have said that openly. The AI revenue has a lower gross margin than our obviously, the rest of the business, including software, of course. But we expect the rate of growth to offer as we do more and more AI revenue to be so so much that we get the operating leverage on our operating spending that operating margin won't deliver dollars that are still a high level of growth from what it has been. So we expect operating leverage to benefit us at the operating margin level even as gross margin will start to deteriorate. High level.\nKirsten Spears: No. I think Hock said that fairly. And the second half of the year when we do start shipping more systems, the situation is straightforward. We'll be passing through more components that are not ours. So think of it similar to the XPUs where we have memory on those XPUs, we're passing through those costs. We'll be passing through more costs within the rack. And so those gross margins will be lower. However, overall, the way Hock said it, gross margin dollars will go up. Margins will go down. Operating margins, because we have leverage, operating margin dollars will go up, but the margin itself, a percentage of revenues will come down a bit. But we're not I mean, we'll guide closer to you know, the end of the year for that.\nStacy Rasgon: Got it. Thank you, guys.\nOperator: One moment for our next question. That will come from the line of Jim Schneider with Goldman Sachs. Your line is open.\nJim Schneider: Good afternoon. Thanks for taking my question. Hock, I was wondering if you might care to calibrate your expectations for AI revenue in fiscal 2026 a little bit more closely. I believe you talked about acceleration in fiscal 2026 off of the 65% growth rate you did in fiscal 2025. And then you're guiding to 100% growth for Q1. So I'm just wondering if the Q1 is a good jumping-off point for the growth rate you expect for the full year something maybe a little bit less than that? And then maybe if you could separately clarify whether your $1 billion of orders for the fifth customer is indeed OpenAI, which you made a separate announcement about. Thank you.\nHock Tan: Wow. There's a lot of question here. But let me start off with 2026. You know, our backlog is very dynamic these days. As I said. And it is continuing to ramp up. And you're right. We originally, six months ago said maybe year on year, AI revenues would grow in 2026 sixty, 70%. Q1, we doubled. And Q1 2026, today, we're saying it doubled. And when we're looking at it, because all the fresh orders keep coming in, and we give you a milestone of where we are today. Which is $73 billion of backlog to be shipped over the next eighteen months. And we do fully expect as I answered the earlier question, for that $73 billion over the eighteen months, to keep growing, Now it's a moving target. It's a moving number as we move in time. But it will grow. And it's hard for me to pinpoint what 2026 is going to look like precisely. I rather not give you guys any guidance, and that's why we don't give you guidance. But we do give it for Q1. Give it time or give it till Q2, and you're right. It's saying that to us, is it an accelerating trend? And my answer is it's likely to be an accelerating trend. As we progress through 2026. Hope that answers. Your question.\nJim Schneider: Yes. Thank you.\nOperator: One moment for our next question. And that will come from the line of Ben Reitzes with Melius Research. Your line is open.\nBen Reitzes: Yeah. Hey, guys. Thanks a lot. Hey, Hock. I wanted to ask I I'm not sure if the last caller said something on it, but I didn't hear it in the answer was wanted to ask about the OpenAI contract that it's supposed to start in the second half of the year and go through 2029 for 10 gigawatts. I'm gonna assume that that's the that's the fifth customer order there. And I was just wondering if you're still confident in that being a driver. Are there any obstacles to making that a major driver? And when you expect, you know, that to contribute? And your confidence in it. Thanks so much, Hock.\nHock Tan: You didn't hear that answer from my last caller, Jim's question was because I did not answer it I'm not answering it either. The fifth customer. And he's a real customer and he will grow. They are on their multi-year journey to their own XPUs. And let's leave it at that. As far as the OpenAI view that you have, I we appreciate the fact that it is a multiyear journey. That will run through '29 as our press release with OpenAI showed. 10 gigawatts between '26 to more more like '27, 28. 29. Ben. Not 26. It's more like 27, 28, 29, 10 gigawatts. That was the OpenAI discussion. And that's I call it an agreement, an alignment where we're headed with respect to a various respected and valued customer OpenAI.\nBen Reitzes: But that's real interesting.\nHock Tan: We do not expect March in '26. Ah, okay. That's thanks for clarifying that. That's really interesting. Appreciate it.\nOperator: One moment for our next question. And that will come from the line of CJ Muse with Cantor Fitzgerald. Your line is open.\nCJ Muse: Yes. Good afternoon. Thank you for taking the question. I guess, Hock, I wanted to talk about custom silicon and maybe speak to how you expect content to grow for broad generation to generation. And as part of that, you know, your competitor announced CPX offering essentially accelerator for an accelerator for massive context windows. I'm curious if you see it a broadening opportunity you know, for your existing five customers to have multiple XPU offerings. Thanks so much.\nHock Tan: Thank you. No. Yeah. It's you hit it right on. I mean, the nice thing about a custom accelerator is you try not to do one size fits all. And do and generationally. Each of these five customers now can create their version of chip of an XPU customer accelerator for training. And inference. And it basically is almost two parallel tracks going on almost simultaneously for each of them. So I would have plenty of versions to deal with. I do not need to create any more version. We got plenty of different content out there. Just on the basis of creating these customer accelerators. And by the way, when you do customer accelerators, you tend to put more hardware in that unique differentiated versus trying to make it work on software. And creating kernels into software. I know that's very tricky too. But think about the difference where you can create in hardware those sparse call data routers. Versus the dense matrix multipliers all in one same chip. And that's one of many one of just one example of what creating customer accelerators is letting us do. Over that matter, a variation in how much memory capacity or memory bandwidth from for the same customer from chip to chip just because even in inference, want to do more reasoning, first decoding, versus something else, like prefilled. So you literally start to create different hardware for different and run your workloads. aspect of how you want to train or inference It's a very fascinating area. And we are seeing a lot of variations and multiple chips. For each of our customers. Thank you.\nOperator: One moment for our next question. And that will come from the line of Harsh Kumar with Piper Sandler. Your line is open.\nHarsh Kumar: Yeah, Hock and team. First of all, congratulations on some pretty stunning numbers. I've got an easy one and a more strategic one. The easy one is, your guide in AI Hock and Kirsten is calling for almost $1.7 billion of sequential growth. I was curious, maybe you could talk about the diversity of the growth between the three existing customers. Is it pretty well spread out? All of them growing? Or is one sort of driving much of the growth? And then how strategically one of your competitors bought a photonic fabric company recently I was curious about your take on that technology and if you think it's disruptive or you think it's just gimmickry at this point in time.\nHock Tan: I like the way you address this question because the way that you address the question to me is almost hesitant. Thank you. I appreciate that. But on your first part, yeah, we are driving growth and we it began to feel like this thing never ends. And it's a real mixed bag. Of existing customers and on existing XPUs And I'll be part of it as XPUs that we're seeing. And that's not to slow down the fact that as I indicated in my remarks and commented on the demand for switches. Not just among six, not among five. Switches. The demand for our latest 1.6 terabit per second DSPs. That enables optical interconnects for scale out. Particularly? It's just very, very strong. And by extension, demand for the optical components like lasers, pin diodes, just going nuts. All that come together. Now, all that is small dollar, relatively lesser dollars when it comes to XPUs, as you probably guessed. I mean, no. Of this to give you a sense, maybe let me look at it on a backlog side. Of the $73 billion or AI revenue backlog over the next eighteen months, I talked about maybe $20 billion of it is everything else. The rest, is XPUs. Hope that gives you a sense for what the mix is. That's not to say that the rest is still $20 billion. That's not small. By any means. So we value that. So when you talk about your next question of silicon photonics, and as a means to create basically much better, more efficient, lower power interconnects in not just scale out, but hopefully scale up Yeah. I could see a point in time in the future when silicon photonics methods as the only way to do it. We're not quite there yet. But we have the technology and we continue to develop the technology even as each time we develop it first for 400 gigabit then we going on to nine 800 gigabit bandwidth Not ready for it yet. So and even we have the product, and we're now doing it for 1.6 terabit bandwidth to create silicon photonics switches silicon photonics interconnects Not even sure it will get fully deployed because you know, engineers, our engineers, our peer, and the peers we have out there, was somehow trying to find a way to still do try to do scale up within a rack in copper as long as possible. And in scale up in no pluggable optics. The final final straw is when you can't do it well in pluggable optics. Of course, when you can't do it even in copper, then you're right. You go to silicon photonics. And it will happen. And we're ready for it. Just saying, not anytime soon.\nHarsh Kumar: Thank you, Hock.\nOperator: One moment for our next question. That will come from the line of Carl Ackerman with BNP Paribas. Your line is open.\nCarl Ackerman: Yes. Thank you. Could you speak to the supply chain resiliency and visibility you have with your key material suppliers, particularly co-ops as you not only support your existing customer programs, but the two new custom compute processors that you announced since your quarter. I guess what I can get at is you also happen to address the very large subset of networking and compute AI supply chains. You've talked about record backlog. If you were to pin some of the bottlenecks that you have, the areas that you're aiming to address and mitigate from supply chain bottlenecks, what would they be and how do you see that ameliorating into '26? Thank you.\nHock Tan: It's across the board. Typically. I mean, it's we are very fortunate in some ways that we have the product technology and the operating business lines. To create multiple key leading-edge components that enable today's state-of-the-art AI data centers. I mean, our DSP as I said earlier, is now at 1.6 terabits per second. That's the leading edge. Connectivity for bandwidth for this for the top of the top of the heap, XPU, and even GPU. And we intend to be that way. And we have the lasers EMLs, VCSELs, CWL lasers that go with it. So it's fortunate that we have all this. And the key comp active components that go with it and we see it very quick early, and we expand the capacity as we do the design to match it. And along this is the long answer to what I'm trying to get at, which is I think we are of any of any of this data center suppliers of the system racks not counting the power the power shell and all that. Now that's hard to get beyond us. On the PowerShell and the transformers and the gas turbines. If you just look at the rack, the systems on AI, we probably have a good handle on where the bottlenecks are because sometimes we are part of the bottlenecks. Which we then want to get rid to resolve. So we feel pretty good about that. Through 2026.\nCarl Ackerman: Thank you.\nHock Tan: One moment for our next question.\nOperator: That will come from the line of Christopher Rolland with Susquehanna. Your line is open.\nChristopher Rolland: Hi, thanks for the question. Just first a clarification and then my question. And sorry to come back to this issue, but if I understand you correctly, Hock, I think you were saying that OpenAI would be a general agreement so it's not binding maybe similar to the agreements with both NVIDIA and AMD. And then secondly, you talked about flat non-AI semiconductor revenue Maybe what's going on there? Is there still an inventory overhang in and what could what what do we need to get that going again? Do you see growth, eventually, eventually in that business? Thank you.\nHock Tan: Well, on the non-AI semiconductor, we see broadband literally recovering very well. And we do not see the others No. We see stability. We do not see a sharp recovery that is sustainable yet. So I guess given a one a couple more quarters, but we do not see any further deterioration in demand. And it's more I think maybe the ops and AI is sucking the ops oxygen a lot out of enterprise spending elsewhere. And hyperscaler spending elsewhere. We do not see it getting any worse. We do not see it recovering very quickly. With the exception of broadband. That's a simple summary of non-AI. With respect to OpenAI now before diving into the I'm just telling you what that 10 gigawatt announcement is all about. Separately, the journey with them on the customer accelerated progresses at a very advanced stage and will happen very, very quickly. And it's and we will have a committed element to this whole thing. And I won't. But what we I was articulating earlier was the 10 gigawatt announcement. And that 10 gigawatt announcement is an agreement to be aligned on developing 10 gigawatts for OpenAI over 27 to 29 time frame. I said, that's different from the XPU program we're developing with them.\nChristopher Rolland: I see. Thank you very much.\nOperator: Thank you. And we do have time for one final question, and that will come from the line of Joe Moore with Morgan Stanley. Your line is open.\nJoe Moore: Great. Thank you very much. So if you have $21 billion of rack revenue in 2026, I guess, do we stay at that run rate beyond that? Are you gonna continue to sell racks, or does that sort of that type of business make shift over time? And I'm really just trying to figure out the percentage of your eighteen-month backlog that's actually full systems at this point.\nHock Tan: Well, it's an interesting question. And while that question basically comes to how much compute capacity is needed by our customers over the next as I say, over the period beyond eighteen months. And your guess is probably as good as mine based on what we all know out there. Which is really what it relates to. But if they need more, then you see that continuing even larger. If they do not need it, then probably it won't. But so one is what we're trying to indicate is that the demand we are seeing over that period of time right now.\nOperator: Thank you. I would now like to turn the call back over to Ji Yoo for any closing remarks.\nJi Yoo: Thank you, operator. This quarter, Broadcom will be presenting at the New Street Research Virtual AI Big Ideas Conference on Monday, 12/15/2025. Broadcom currently plans to report its earnings for the 2026 after close of market on Wednesday, 03/04/2026. A public webcast of Broadcom's earnings conference That will conclude our earnings call today. Thank you all for joining.\nOperator: This concludes today's program. Thank you all for participating. You may now disconnect."}]